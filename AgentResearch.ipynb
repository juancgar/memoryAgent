{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyO6eWMb+Yq8lPpvosFFXtBd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/juancgar/memoryAgent/blob/main/AgentResearch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install langchain_openai"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3VCnEFkjwMBu",
        "outputId": "61a0ce35-a2fd-4879-9ef7-f8ecc63f2efd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain_openai in /usr/local/lib/python3.10/dist-packages (0.3.0)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.29 in /usr/local/lib/python3.10/dist-packages (from langchain_openai) (0.3.29)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.58.1 in /usr/local/lib/python3.10/dist-packages (from langchain_openai) (1.59.7)\n",
            "Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.10/dist-packages (from langchain_openai) (0.8.0)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.29->langchain_openai) (6.0.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.29->langchain_openai) (1.33)\n",
            "Requirement already satisfied: langsmith<0.3,>=0.1.125 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.29->langchain_openai) (0.2.3)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.29->langchain_openai) (24.2)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.29->langchain_openai) (2.10.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.29->langchain_openai) (9.0.0)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.29->langchain_openai) (4.12.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.58.1->langchain_openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.58.1->langchain_openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.58.1->langchain_openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.58.1->langchain_openai) (0.8.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.58.1->langchain_openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.58.1->langchain_openai) (4.67.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.7->langchain_openai) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.7->langchain_openai) (2.32.3)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.58.1->langchain_openai) (3.10)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.58.1->langchain_openai) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.58.1->langchain_openai) (2024.12.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.58.1->langchain_openai) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.58.1->langchain_openai) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.29->langchain_openai) (3.0.0)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.125->langchain-core<0.4.0,>=0.3.29->langchain_openai) (3.10.12)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.125->langchain-core<0.4.0,>=0.3.29->langchain_openai) (1.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.29->langchain_openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.29->langchain_openai) (2.27.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain_openai) (3.4.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain_openai) (2.2.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install git+https://github.com/brandonstarxel/chunking_evaluation.git"
      ],
      "metadata": {
        "id": "76ZxACveUktP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bedaca68-a64f-44dc-e47c-ab1635314d33"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/brandonstarxel/chunking_evaluation.git\n",
            "  Cloning https://github.com/brandonstarxel/chunking_evaluation.git to /tmp/pip-req-build-0kvi4jgh\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/brandonstarxel/chunking_evaluation.git /tmp/pip-req-build-0kvi4jgh\n",
            "  Resolved https://github.com/brandonstarxel/chunking_evaluation.git to commit a503ddf9af2275d492881c4fa6918d99ae37ab50\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (from chunking_evaluation==0.1.0) (0.8.0)\n",
            "Requirement already satisfied: fuzzywuzzy in /usr/local/lib/python3.10/dist-packages (from chunking_evaluation==0.1.0) (0.18.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from chunking_evaluation==0.1.0) (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from chunking_evaluation==0.1.0) (1.26.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from chunking_evaluation==0.1.0) (4.67.1)\n",
            "Requirement already satisfied: chromadb in /usr/local/lib/python3.10/dist-packages (from chunking_evaluation==0.1.0) (0.6.3)\n",
            "Requirement already satisfied: python-Levenshtein in /usr/local/lib/python3.10/dist-packages (from chunking_evaluation==0.1.0) (0.26.1)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (from chunking_evaluation==0.1.0) (1.59.7)\n",
            "Requirement already satisfied: anthropic in /usr/local/lib/python3.10/dist-packages (from chunking_evaluation==0.1.0) (0.43.0)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.10/dist-packages (from chunking_evaluation==0.1.0) (24.3.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from anthropic->chunking_evaluation==0.1.0) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from anthropic->chunking_evaluation==0.1.0) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from anthropic->chunking_evaluation==0.1.0) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from anthropic->chunking_evaluation==0.1.0) (0.8.2)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from anthropic->chunking_evaluation==0.1.0) (2.10.3)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from anthropic->chunking_evaluation==0.1.0) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.10/dist-packages (from anthropic->chunking_evaluation==0.1.0) (4.12.2)\n",
            "Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.10/dist-packages (from chromadb->chunking_evaluation==0.1.0) (1.2.2.post1)\n",
            "Requirement already satisfied: chroma-hnswlib==0.7.6 in /usr/local/lib/python3.10/dist-packages (from chromadb->chunking_evaluation==0.1.0) (0.7.6)\n",
            "Requirement already satisfied: fastapi>=0.95.2 in /usr/local/lib/python3.10/dist-packages (from chromadb->chunking_evaluation==0.1.0) (0.115.6)\n",
            "Requirement already satisfied: uvicorn>=0.18.3 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb->chunking_evaluation==0.1.0) (0.34.0)\n",
            "Requirement already satisfied: posthog>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from chromadb->chunking_evaluation==0.1.0) (3.8.3)\n",
            "Requirement already satisfied: onnxruntime>=1.14.1 in /usr/local/lib/python3.10/dist-packages (from chromadb->chunking_evaluation==0.1.0) (1.20.1)\n",
            "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from chromadb->chunking_evaluation==0.1.0) (1.29.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from chromadb->chunking_evaluation==0.1.0) (1.29.0)\n",
            "Requirement already satisfied: opentelemetry-instrumentation-fastapi>=0.41b0 in /usr/local/lib/python3.10/dist-packages (from chromadb->chunking_evaluation==0.1.0) (0.50b0)\n",
            "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from chromadb->chunking_evaluation==0.1.0) (1.29.0)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from chromadb->chunking_evaluation==0.1.0) (0.21.0)\n",
            "Requirement already satisfied: pypika>=0.48.9 in /usr/local/lib/python3.10/dist-packages (from chromadb->chunking_evaluation==0.1.0) (0.48.9)\n",
            "Requirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.10/dist-packages (from chromadb->chunking_evaluation==0.1.0) (7.7.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from chromadb->chunking_evaluation==0.1.0) (6.4.5)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.10/dist-packages (from chromadb->chunking_evaluation==0.1.0) (1.69.0)\n",
            "Requirement already satisfied: bcrypt>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from chromadb->chunking_evaluation==0.1.0) (4.2.1)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from chromadb->chunking_evaluation==0.1.0) (0.15.1)\n",
            "Requirement already satisfied: kubernetes>=28.1.0 in /usr/local/lib/python3.10/dist-packages (from chromadb->chunking_evaluation==0.1.0) (31.0.0)\n",
            "Requirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.10/dist-packages (from chromadb->chunking_evaluation==0.1.0) (9.0.0)\n",
            "Requirement already satisfied: PyYAML>=6.0.0 in /usr/local/lib/python3.10/dist-packages (from chromadb->chunking_evaluation==0.1.0) (6.0.2)\n",
            "Requirement already satisfied: mmh3>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from chromadb->chunking_evaluation==0.1.0) (5.0.1)\n",
            "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.10/dist-packages (from chromadb->chunking_evaluation==0.1.0) (3.10.12)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from chromadb->chunking_evaluation==0.1.0) (13.9.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->chunking_evaluation==0.1.0) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->chunking_evaluation==0.1.0) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->chunking_evaluation==0.1.0) (2024.2)\n",
            "Requirement already satisfied: Levenshtein==0.26.1 in /usr/local/lib/python3.10/dist-packages (from python-Levenshtein->chunking_evaluation==0.1.0) (0.26.1)\n",
            "Requirement already satisfied: rapidfuzz<4.0.0,>=3.9.0 in /usr/local/lib/python3.10/dist-packages (from Levenshtein==0.26.1->python-Levenshtein->chunking_evaluation==0.1.0) (3.11.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken->chunking_evaluation==0.1.0) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken->chunking_evaluation==0.1.0) (2.32.3)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->anthropic->chunking_evaluation==0.1.0) (3.10)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->anthropic->chunking_evaluation==0.1.0) (1.2.2)\n",
            "Requirement already satisfied: packaging>=19.1 in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb->chunking_evaluation==0.1.0) (24.2)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb->chunking_evaluation==0.1.0) (1.2.0)\n",
            "Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb->chunking_evaluation==0.1.0) (2.2.1)\n",
            "Requirement already satisfied: starlette<0.42.0,>=0.40.0 in /usr/local/lib/python3.10/dist-packages (from fastapi>=0.95.2->chromadb->chunking_evaluation==0.1.0) (0.41.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->anthropic->chunking_evaluation==0.1.0) (2024.12.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->anthropic->chunking_evaluation==0.1.0) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->anthropic->chunking_evaluation==0.1.0) (0.14.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb->chunking_evaluation==0.1.0) (1.17.0)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb->chunking_evaluation==0.1.0) (2.27.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb->chunking_evaluation==0.1.0) (1.8.0)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb->chunking_evaluation==0.1.0) (1.3.1)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb->chunking_evaluation==0.1.0) (3.2.2)\n",
            "Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb->chunking_evaluation==0.1.0) (2.2.3)\n",
            "Requirement already satisfied: durationpy>=0.7 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb->chunking_evaluation==0.1.0) (0.9)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb->chunking_evaluation==0.1.0) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb->chunking_evaluation==0.1.0) (24.3.25)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb->chunking_evaluation==0.1.0) (5.29.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb->chunking_evaluation==0.1.0) (1.13.1)\n",
            "Requirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-api>=1.2.0->chromadb->chunking_evaluation==0.1.0) (1.2.15)\n",
            "Requirement already satisfied: importlib-metadata<=8.5.0,>=6.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-api>=1.2.0->chromadb->chunking_evaluation==0.1.0) (8.5.0)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb->chunking_evaluation==0.1.0) (1.66.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.29.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb->chunking_evaluation==0.1.0) (1.29.0)\n",
            "Requirement already satisfied: opentelemetry-proto==1.29.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb->chunking_evaluation==0.1.0) (1.29.0)\n",
            "Requirement already satisfied: opentelemetry-instrumentation-asgi==0.50b0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb->chunking_evaluation==0.1.0) (0.50b0)\n",
            "Requirement already satisfied: opentelemetry-instrumentation==0.50b0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb->chunking_evaluation==0.1.0) (0.50b0)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.50b0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb->chunking_evaluation==0.1.0) (0.50b0)\n",
            "Requirement already satisfied: opentelemetry-util-http==0.50b0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb->chunking_evaluation==0.1.0) (0.50b0)\n",
            "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation==0.50b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb->chunking_evaluation==0.1.0) (1.17.0)\n",
            "Requirement already satisfied: asgiref~=3.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation-asgi==0.50b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb->chunking_evaluation==0.1.0) (3.8.1)\n",
            "Requirement already satisfied: monotonic>=1.5 in /usr/local/lib/python3.10/dist-packages (from posthog>=2.4.0->chromadb->chunking_evaluation==0.1.0) (1.6)\n",
            "Requirement already satisfied: backoff>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from posthog>=2.4.0->chromadb->chunking_evaluation==0.1.0) (2.2.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->anthropic->chunking_evaluation==0.1.0) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->anthropic->chunking_evaluation==0.1.0) (2.27.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->chunking_evaluation==0.1.0) (3.4.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->chromadb->chunking_evaluation==0.1.0) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->chromadb->chunking_evaluation==0.1.0) (2.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers>=0.13.2->chromadb->chunking_evaluation==0.1.0) (0.27.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->chromadb->chunking_evaluation==0.1.0) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->chromadb->chunking_evaluation==0.1.0) (1.5.4)\n",
            "Requirement already satisfied: httptools>=0.6.3 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb->chunking_evaluation==0.1.0) (0.6.4)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb->chunking_evaluation==0.1.0) (1.0.1)\n",
            "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb->chunking_evaluation==0.1.0) (0.21.0)\n",
            "Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb->chunking_evaluation==0.1.0) (1.0.4)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb->chunking_evaluation==0.1.0) (14.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb->chunking_evaluation==0.1.0) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb->chunking_evaluation==0.1.0) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb->chunking_evaluation==0.1.0) (4.9)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb->chunking_evaluation==0.1.0) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb->chunking_evaluation==0.1.0) (2024.10.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata<=8.5.0,>=6.0->opentelemetry-api>=1.2.0->chromadb->chunking_evaluation==0.1.0) (3.21.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb->chunking_evaluation==0.1.0) (0.1.2)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.10/dist-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb->chunking_evaluation==0.1.0) (10.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb->chunking_evaluation==0.1.0) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb->chunking_evaluation==0.1.0) (0.6.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pypdf\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wdjqiVxpUwxa",
        "outputId": "ab4b55bc-e939-4da7-a729-abd46d426902"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.10/dist-packages (5.1.0)\n",
            "Requirement already satisfied: typing_extensions>=4.0 in /usr/local/lib/python3.10/dist-packages (from pypdf) (4.12.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install langchain_community"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SOswS0H1Uff3",
        "outputId": "a8b5ae35-c53f-463a-93e7-5e7bf87f12db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain_community in /usr/local/lib/python3.10/dist-packages (0.3.14)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (2.0.36)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (3.11.10)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.6.7)\n",
            "Requirement already satisfied: httpx-sse<0.5.0,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.4.0)\n",
            "Requirement already satisfied: langchain<0.4.0,>=0.3.14 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.3.14)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.29 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.3.29)\n",
            "Requirement already satisfied: langsmith<0.3,>=0.1.125 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.2.3)\n",
            "Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (1.26.4)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (2.7.1)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (9.0.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.18.3)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.25.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\n",
            "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.3 in /usr/local/lib/python3.10/dist-packages (from langchain<0.4.0,>=0.3.14->langchain_community) (0.3.3)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from langchain<0.4.0,>=0.3.14->langchain_community) (2.10.3)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.29->langchain_community) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.29->langchain_community) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.29->langchain_community) (4.12.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.125->langchain_community) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.125->langchain_community) (3.10.12)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.125->langchain_community) (1.0.0)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain_community) (1.0.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (2024.12.14)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.1.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain_community) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain_community) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain_community) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.29->langchain_community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.14->langchain_community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.14->langchain_community) (2.27.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.0.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain_community) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain_community) (1.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -U weaviate-client"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "v9okZ5t0nJnu",
        "outputId": "929aace7-fa74-43b0-ae79-1657f4e16ccb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: weaviate-client in /usr/local/lib/python3.10/dist-packages (4.10.4)\n",
            "Requirement already satisfied: httpx<0.29.0,>=0.26.0 in /usr/local/lib/python3.10/dist-packages (from weaviate-client) (0.28.1)\n",
            "Requirement already satisfied: validators==0.34.0 in /usr/local/lib/python3.10/dist-packages (from weaviate-client) (0.34.0)\n",
            "Requirement already satisfied: authlib<1.3.2,>=1.2.1 in /usr/local/lib/python3.10/dist-packages (from weaviate-client) (1.3.1)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.8.0 in /usr/local/lib/python3.10/dist-packages (from weaviate-client) (2.10.3)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.66.2 in /usr/local/lib/python3.10/dist-packages (from weaviate-client) (1.69.0)\n",
            "Requirement already satisfied: grpcio-tools<2.0.0,>=1.66.2 in /usr/local/lib/python3.10/dist-packages (from weaviate-client) (1.69.0)\n",
            "Requirement already satisfied: grpcio-health-checking<2.0.0,>=1.66.2 in /usr/local/lib/python3.10/dist-packages (from weaviate-client) (1.69.0)\n",
            "Requirement already satisfied: cryptography in /usr/local/lib/python3.10/dist-packages (from authlib<1.3.2,>=1.2.1->weaviate-client) (43.0.3)\n",
            "Requirement already satisfied: protobuf<6.0dev,>=5.26.1 in /usr/local/lib/python3.10/dist-packages (from grpcio-health-checking<2.0.0,>=1.66.2->weaviate-client) (5.29.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from grpcio-tools<2.0.0,>=1.66.2->weaviate-client) (75.1.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<0.29.0,>=0.26.0->weaviate-client) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<0.29.0,>=0.26.0->weaviate-client) (2024.12.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<0.29.0,>=0.26.0->weaviate-client) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx<0.29.0,>=0.26.0->weaviate-client) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<0.29.0,>=0.26.0->weaviate-client) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.8.0->weaviate-client) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.8.0->weaviate-client) (2.27.1)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.8.0->weaviate-client) (4.12.2)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<0.29.0,>=0.26.0->weaviate-client) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<0.29.0,>=0.26.0->weaviate-client) (1.2.2)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography->authlib<1.3.2,>=1.2.1->weaviate-client) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography->authlib<1.3.2,>=1.2.1->weaviate-client) (2.22)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "from google.colab import userdata\n",
        "\n",
        "\n",
        "llm = ChatOpenAI(temperature=0.7, model=\"gpt-4o\", api_key=userdata.get('OPENAI_API_KEY'))"
      ],
      "metadata": {
        "id": "UAyGxPlZiekj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "\n",
        "# Define System Prompt\n",
        "system_prompt = SystemMessage(\"You are a helpful AI Assistant. Answer the User's queries succinctly in one sentence.\")\n",
        "\n",
        "# Start Storage for Historical Message History\n",
        "messages = [system_prompt]\n",
        "\n",
        "while True:\n",
        "\n",
        "    # Get User's Message\n",
        "    user_message = HumanMessage(input(\"\\nUser: \"))\n",
        "\n",
        "    if user_message.content.lower() == \"exit\":\n",
        "        break\n",
        "\n",
        "    else:\n",
        "        # Extend Messages List With User Message\n",
        "        messages.append(user_message)\n",
        "\n",
        "    # Pass Entire Message Sequence to LLM to Generate Response\n",
        "    response = llm.invoke(messages)\n",
        "\n",
        "    print(\"\\nAI Message: \", response.content)\n",
        "\n",
        "    # Add AI's Response to Message List\n",
        "    messages.append(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HwUBhzhS0Cjf",
        "outputId": "dc33c8d3-6eea-4512-d079-cc557d777d0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "User: Hi good night\n",
            "\n",
            "AI Message:  Good night! How can I assist you today?\n",
            "\n",
            "User: I want to learn more about marine biology, can you point me to good papers to start investigating about coral reef and the effect of global warming on it?\n",
            "\n",
            "AI Message:  Certainly! A good starting point is the paper \"Coral Reefs Under Rapid Climate Change and Ocean Acidification\" by Hoegh-Guldberg et al., published in Science, and \"Climate Change, Human Impacts, and the Resilience of Coral Reefs\" by Hughes et al., also in Science.\n",
            "\n",
            "User: can you summarise in 100 words the first paper\n",
            "\n",
            "AI Message:  The paper \"Coral Reefs Under Rapid Climate Change and Ocean Acidification\" by Hoegh-Guldberg et al. discusses the vulnerability of coral reefs to climate change and ocean acidification, driven by increased atmospheric CO2 levels. It highlights how rising sea temperatures lead to coral bleaching and mortality, while acidification reduces calcification rates, weakening reef structures. The authors emphasize the critical threshold of a 1-2°C increase in global temperatures, beyond which coral reefs may not recover. The paper calls for urgent global efforts to reduce CO2 emissions and implement conservation strategies to enhance the resilience of coral ecosystems against these environmental changes.\n",
            "\n",
            "User: exit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import JsonOutputParser\n",
        "\n",
        "reflection_prompt_template = \"\"\"\n",
        "You are analyzing conversations about research papers to create memories that will help guide future interactions. Your task is to extract key elements that would be most helpful when encountering similar academic discussions in the future.\n",
        "\n",
        "Review the conversation and create a memory reflection following these rules:\n",
        "\n",
        "1. For any field where you don't have enough information or the field isn't relevant, use \"N/A\"\n",
        "2. Be extremely concise - each string should be one clear, actionable sentence\n",
        "3. Focus only on information that would be useful for handling similar future conversations\n",
        "4. Context_tags should be specific enough to match similar situations but general enough to be reusable\n",
        "\n",
        "Output valid JSON in exactly this format:\n",
        "{{\n",
        "    \"context_tags\": [              // 2-4 keywords that would help identify similar future conversations\n",
        "        string,                    // Use field-specific terms like \"deep_learning\", \"methodology_question\", \"results_interpretation\"\n",
        "        ...\n",
        "    ],\n",
        "    \"conversation_summary\": string, // One sentence describing what the conversation accomplished\n",
        "    \"what_worked\": string,         // Most effective approach or strategy used in this conversation\n",
        "    \"what_to_avoid\": string        // Most important pitfall or ineffective approach to avoid\n",
        "}}\n",
        "\n",
        "Examples:\n",
        "- Good context_tags: [\"transformer_architecture\", \"attention_mechanism\", \"methodology_comparison\"]\n",
        "- Bad context_tags: [\"machine_learning\", \"paper_discussion\", \"questions\"]\n",
        "\n",
        "- Good conversation_summary: \"Explained how the attention mechanism in the BERT paper differs from traditional transformer architectures\"\n",
        "- Bad conversation_summary: \"Discussed a machine learning paper\"\n",
        "\n",
        "- Good what_worked: \"Using analogies from matrix multiplication to explain attention score calculations\"\n",
        "- Bad what_worked: \"Explained the technical concepts well\"\n",
        "\n",
        "- Good what_to_avoid: \"Diving into mathematical formulas before establishing user's familiarity with linear algebra fundamentals\"\n",
        "- Bad what_to_avoid: \"Used complicated language\"\n",
        "\n",
        "Additional examples for different research scenarios:\n",
        "\n",
        "Context tags examples:\n",
        "- [\"experimental_design\", \"control_groups\", \"methodology_critique\"]\n",
        "- [\"statistical_significance\", \"p_value_interpretation\", \"sample_size\"]\n",
        "- [\"research_limitations\", \"future_work\", \"methodology_gaps\"]\n",
        "\n",
        "Conversation summary examples:\n",
        "- \"Clarified why the paper's cross-validation approach was more robust than traditional hold-out methods\"\n",
        "- \"Helped identify potential confounding variables in the study's experimental design\"\n",
        "\n",
        "What worked examples:\n",
        "- \"Breaking down complex statistical concepts using visual analogies and real-world examples\"\n",
        "- \"Connecting the paper's methodology to similar approaches in related seminal papers\"\n",
        "\n",
        "What to avoid examples:\n",
        "- \"Assuming familiarity with domain-specific jargon without first checking understanding\"\n",
        "- \"Over-focusing on mathematical proofs when the user needed intuitive understanding\"\n",
        "\n",
        "Do not include any text outside the JSON object in your response.\n",
        "\n",
        "Here is the prior conversation:\n",
        "\n",
        "{conversation}\n",
        "\"\"\"\n",
        "\n",
        "reflection_prompt = ChatPromptTemplate.from_template(reflection_prompt_template)\n",
        "\n",
        "reflect = reflection_prompt | llm | JsonOutputParser()"
      ],
      "metadata": {
        "id": "5yMXs8jvB4cm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def format_conversation(messages):\n",
        "\n",
        "    # Create an empty list placeholder\n",
        "    conversation = []\n",
        "\n",
        "    # Start from index 1 to skip the first system message\n",
        "    for message in messages[1:]:\n",
        "        conversation.append(f\"{message.type.upper()}: {message.content}\")\n",
        "\n",
        "    # Join with newlines\n",
        "    return \"\\n\".join(conversation)\n",
        "\n",
        "conversation = format_conversation(messages)\n",
        "\n",
        "print(conversation)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pBt0mC7tDgWi",
        "outputId": "2b8aeb45-891d-4153-c656-396ea4a65767"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HUMAN: Hi good night\n",
            "AI: Good night! How can I assist you today?\n",
            "HUMAN: I want to learn more about marine biology, can you point me to good papers to start investigating about coral reef and the effect of global warming on it?\n",
            "AI: Certainly! A good starting point is the paper \"Coral Reefs Under Rapid Climate Change and Ocean Acidification\" by Hoegh-Guldberg et al., published in Science, and \"Climate Change, Human Impacts, and the Resilience of Coral Reefs\" by Hughes et al., also in Science.\n",
            "HUMAN: can you summarise in 100 words the first paper\n",
            "AI: The paper \"Coral Reefs Under Rapid Climate Change and Ocean Acidification\" by Hoegh-Guldberg et al. discusses the vulnerability of coral reefs to climate change and ocean acidification, driven by increased atmospheric CO2 levels. It highlights how rising sea temperatures lead to coral bleaching and mortality, while acidification reduces calcification rates, weakening reef structures. The authors emphasize the critical threshold of a 1-2°C increase in global temperatures, beyond which coral reefs may not recover. The paper calls for urgent global efforts to reduce CO2 emissions and implement conservation strategies to enhance the resilience of coral ecosystems against these environmental changes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reflection = reflect.invoke({\"conversation\": conversation})\n",
        "\n",
        "print(reflection)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CR2pZGXlD2ZS",
        "outputId": "197d7dfd-ba5c-4914-fe96-ed50a694a1bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'context_tags': ['marine_biology', 'coral_reef', 'climate_change_impact'], 'conversation_summary': \"Provided a summary of the paper on coral reefs' vulnerability to climate change and ocean acidification.\", 'what_worked': 'Recommending well-regarded papers and providing concise summaries upon request.', 'what_to_avoid': 'N/A'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import weaviate\n",
        "from weaviate.classes.init import Auth\n",
        "import os\n",
        "\n",
        "# Best practice: store your credentials in environment variables\n",
        "wcd_url = userdata.get('WCD_UR')\n",
        "wcd_api_key = userdata.get('WCD_API_KEY')\n",
        "openai_api_key = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "client = weaviate.connect_to_weaviate_cloud(\n",
        "    cluster_url=wcd_url,  # Replace with your Weaviate Cloud URL\n",
        "    auth_credentials=Auth.api_key(wcd_api_key),  # Replace with your Weaviate Cloud key\n",
        "    headers={'X-OpenAI-Api-key': openai_api_key}  # Replace with your OpenAI API key\n",
        ")"
      ],
      "metadata": {
        "id": "OB-mo2IT7fBv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Connected to Weviate: \", client.is_ready())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Fu31Pb1nGdp",
        "outputId": "61adba03-306e-4635-c4a8-08eb8e12da17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Connected to Weviate:  True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from weaviate.classes.config import Property, DataType, Configure, Tokenization\n",
        "\n",
        "client.collections.delete(\"episodic_memory\")\n",
        "client.collections.create(\n",
        "    name=\"episodic_memory\",\n",
        "    description=\"Collection containing historical chat interactions and takeaways.\",\n",
        "    vectorizer_config=Configure.Vectorizer.text2vec_openai(), # Change to text2vec-openai for Weaviate Cloud\n",
        "    properties=[\n",
        "        Property(name=\"conversation\", data_type=DataType.TEXT),\n",
        "        Property(name=\"context_tags\", data_type=DataType.TEXT_ARRAY),\n",
        "        Property(name=\"conversation_summary\", data_type=DataType.TEXT),\n",
        "        Property(name=\"what_worked\", data_type=DataType.TEXT),\n",
        "        Property(name=\"what_to_avoid\", data_type=DataType.TEXT),\n",
        "\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nd33R8stoQFB",
        "outputId": "aaaf994b-85df-426b-c2f2-f8751a089db2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<weaviate.collections.collection.sync.Collection at 0x7d0dc06c34c0>"
            ]
          },
          "metadata": {},
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def add_episodic_memory(messages, vdb_client):\n",
        "\n",
        "    # Format Messages\n",
        "    conversation = format_conversation(messages)\n",
        "\n",
        "    # Create Reflection\n",
        "    reflection = reflect.invoke({\"conversation\": conversation})\n",
        "\n",
        "    # Load Database Collection\n",
        "    episodic_memory = vdb_client.collections.get(\"episodic_memory\")\n",
        "\n",
        "    # Insert Entry Into Collection\n",
        "    episodic_memory.data.insert({\n",
        "        \"conversation\": conversation,\n",
        "        \"context_tags\": reflection['context_tags'],\n",
        "        \"conversation_summary\": reflection['conversation_summary'],\n",
        "        \"what_worked\": reflection['what_worked'],\n",
        "        \"what_to_avoid\": reflection['what_to_avoid'],\n",
        "    })\n",
        "add_episodic_memory(messages, client)\n"
      ],
      "metadata": {
        "id": "6dcmlmgmoVg9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def episodic_recall(query, vdb_client):\n",
        "\n",
        "    # Load Database Collection\n",
        "    episodic_memory = vdb_client.collections.get(\"episodic_memory\")\n",
        "\n",
        "    # Hybrid Semantic/BM25 Retrieval\n",
        "    memory = episodic_memory.query.hybrid(\n",
        "        query=query,\n",
        "        alpha=0.5,\n",
        "        limit=1,\n",
        "    )\n",
        "\n",
        "    return memory\n",
        "\n",
        "query = \"Talking about my name\"\n",
        "\n",
        "memory = episodic_recall(query, client)\n",
        "\n",
        "memory.objects[0].properties"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-56zFGEUoatT",
        "outputId": "85c8e5c8-e55a-48be-fd10-c43c1ade2120"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'what_worked': \"Providing specific paper recommendations and a concise summary to guide the user's research.\",\n",
              " 'conversation_summary': 'Recommended and summarized a key paper on the impact of climate change on coral reefs for a beginner in marine biology.',\n",
              " 'context_tags': ['marine_biology', 'climate_change', 'coral_reefs'],\n",
              " 'conversation': 'HUMAN: Hi good night\\nAI: Good night! How can I assist you today?\\nHUMAN: I want to learn more about marine biology, can you point me to good papers to start investigating about coral reef and the effect of global warming on it?\\nAI: Certainly! A good starting point is the paper \"Coral Reefs Under Rapid Climate Change and Ocean Acidification\" by Hoegh-Guldberg et al., published in Science, and \"Climate Change, Human Impacts, and the Resilience of Coral Reefs\" by Hughes et al., also in Science.\\nHUMAN: can you summarise in 100 words the first paper\\nAI: The paper \"Coral Reefs Under Rapid Climate Change and Ocean Acidification\" by Hoegh-Guldberg et al. discusses the vulnerability of coral reefs to climate change and ocean acidification, driven by increased atmospheric CO2 levels. It highlights how rising sea temperatures lead to coral bleaching and mortality, while acidification reduces calcification rates, weakening reef structures. The authors emphasize the critical threshold of a 1-2°C increase in global temperatures, beyond which coral reefs may not recover. The paper calls for urgent global efforts to reduce CO2 emissions and implement conservation strategies to enhance the resilience of coral ecosystems against these environmental changes.',\n",
              " 'what_to_avoid': 'N/A'}"
            ]
          },
          "metadata": {},
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def episodic_system_prompt(query, vdb_client):\n",
        "    # Get new memory\n",
        "    memory = episodic_recall(query, vdb_client)\n",
        "\n",
        "    current_conversation = memory.objects[0].properties['conversation']\n",
        "    # Update memory stores, excluding current conversation from history\n",
        "    if current_conversation not in conversations:\n",
        "        conversations.append(current_conversation)\n",
        "    # conversations.append(memory.objects[0].properties['conversation'])\n",
        "    what_worked.update(memory.objects[0].properties['what_worked'].split('. '))\n",
        "    what_to_avoid.update(memory.objects[0].properties['what_to_avoid'].split('. '))\n",
        "\n",
        "    # Get previous conversations excluding the current one\n",
        "    previous_convos = [conv for conv in conversations[-4:] if conv != current_conversation][-3:]\n",
        "\n",
        "    # Create prompt with accumulated history\n",
        "    episodic_prompt = f\"\"\"You are a helpful AI Assistant. Answer the user's questions to the best of your ability.\n",
        "    You recall similar conversations with the user, here are the details:\n",
        "\n",
        "    Current Conversation Match: {memory.objects[0].properties['conversation']}\n",
        "    Previous Conversations: {' | '.join(previous_convos)}\n",
        "    What has worked well: {' '.join(what_worked)}\n",
        "    What to avoid: {' '.join(what_to_avoid)}\n",
        "\n",
        "    Use these memories as context for your response to the user.\"\"\"\n",
        "\n",
        "    return SystemMessage(content=episodic_prompt)"
      ],
      "metadata": {
        "id": "8nb7LoLmoehL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Simple storage for accumulated memories\n",
        "conversations = []\n",
        "what_worked = set()\n",
        "what_to_avoid = set()\n",
        "\n",
        "# Start Storage for Historical Message History\n",
        "messages = []\n",
        "\n",
        "while True:\n",
        "    # Get User's Message\n",
        "    user_input = input(\"\\nUser: \")\n",
        "    user_message = HumanMessage(content=user_input)\n",
        "\n",
        "    # Generate new system prompt\n",
        "    system_prompt = episodic_system_prompt(user_input, client)\n",
        "\n",
        "    # Reconstruct messages list with new system prompt first\n",
        "    messages = [\n",
        "        system_prompt,  # New system prompt always first\n",
        "        *[msg for msg in messages if not isinstance(msg, SystemMessage)]  # Old messages except system\n",
        "    ]\n",
        "\n",
        "    if user_input.lower() == \"exit\":\n",
        "        add_episodic_memory(messages, client)\n",
        "        print(\"\\n == Conversation Stored in Episodic Memory ==\")\n",
        "        break\n",
        "    if user_input.lower() == \"exit_quiet\":\n",
        "        print(\"\\n == Conversation Exited ==\")\n",
        "        break\n",
        "\n",
        "    # Add current user message\n",
        "    messages.append(user_message)\n",
        "\n",
        "    # Pass Entire Message Sequence to LLM to Generate Response\n",
        "    response = llm.invoke(messages)\n",
        "    print(\"\\nAI Message: \", response.content)\n",
        "\n",
        "    # Add AI's Response to Message List\n",
        "    messages.append(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pBLV9hdmIU99",
        "outputId": "543836a2-fba1-4187-9b6b-8b47092c0cbf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "User: Hi\n",
            "\n",
            "AI Message:  Hello! How can I assist you today?\n",
            "\n",
            "User: I want to investigate about memory in AI\n",
            "\n",
            "AI Message:  Investigating memory in AI is a fascinating topic! There are several aspects to consider, including how AI systems store and recall information, types of memory architectures, and how memory impacts learning and decision-making. Here are a few key concepts and resources that might be helpful:\n",
            "\n",
            "1. **Neural Networks and Memory**: Traditional neural networks have limited memory capabilities. However, architectures like Recurrent Neural Networks (RNNs), Long Short-Term Memory networks (LSTMs), and Gated Recurrent Units (GRUs) are designed to handle sequences and remember information over time.\n",
            "\n",
            "2. **Memory-Augmented Neural Networks (MANNs)**: These are neural networks endowed with an external memory component, such as Neural Turing Machines and Differentiable Neural Computers, which allow them to store and retrieve information in a more flexible and dynamic way.\n",
            "\n",
            "3. **Attention Mechanisms**: Attention is a technique that allows models to focus on specific parts of input data, enhancing memory by selectively processing information. Transformers, which use self-attention mechanisms, are an example of this approach and have been highly successful in NLP tasks.\n",
            "\n",
            "4. **Continual Learning**: This is a field of study focused on enabling AI systems to learn continuously from new data without forgetting previously acquired knowledge. Techniques to mitigate catastrophic forgetting, like elastic weight consolidation, are key here.\n",
            "\n",
            "5. **Key Papers and Resources**:\n",
            "   - \"Neural Turing Machines\" by Graves et al. introduces a model capable of using memory in a neural network.\n",
            "   - \"Attention Is All You Need\" by Vaswani et al. describes the Transformer model, which revolutionized how memory is used in sequence-to-sequence tasks.\n",
            "   - \"Playing Atari with Deep Reinforcement Learning\" by Mnih et al. showcases how memory and reinforcement learning are used in game environments.\n",
            "\n",
            "Would you like more information on any of these topics or another aspect of AI memory?\n",
            "\n",
            "User: exit\n",
            "\n",
            " == Conversation Stored in Episodic Memory ==\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from chunking_evaluation.chunking import RecursiveTokenChunker\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "\n",
        "loader = PyPDFLoader(\"Papers/CoALA_Paper.pdf\")\n",
        "pages = loader.load_and_split()\n",
        "\n",
        "# Combine all page contents into one string\n",
        "document = \" \".join(page.page_content for page in pages)\n",
        "\n",
        "# Set up the chunker with your specified parameters\n",
        "recursive_character_chunker = RecursiveTokenChunker(\n",
        "    chunk_size=800,\n",
        "    chunk_overlap=0,\n",
        "    length_function=len,\n",
        "    separators=[\"\\n\\n\", \"\\n\", \".\", \"?\", \"!\", \" \", \"\"]\n",
        ")\n",
        "\n",
        "# Split the combined text\n",
        "recursive_character_chunks = recursive_character_chunker.split_text(document)"
      ],
      "metadata": {
        "id": "Is1rd2bYUJ25"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(recursive_character_chunks)"
      ],
      "metadata": {
        "id": "7An5r95ZUev-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d972f51-cd82-4e00-80e0-652b22ca051b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "166"
            ]
          },
          "metadata": {},
          "execution_count": 121
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "client.collections.create(\n",
        "    name=\"CoALA_Paper\",\n",
        "    description=\"Collection containing split chunks from the CoALA Paper\",\n",
        "    vectorizer_config=Configure.Vectorizer.text2vec_openai(),\n",
        "    properties=[\n",
        "        Property(name=\"chunk\", data_type=DataType.TEXT),\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "twaL9gt5UcT2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89afbc2f-14a3-4921-bba0-1480fd25abfe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<weaviate.collections.collection.sync.Collection at 0x7d0d6fcf0430>"
            ]
          },
          "metadata": {},
          "execution_count": 123
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Database Collection\n",
        "coala_collection = client.collections.get(\"CoALA_Paper\")\n",
        "\n",
        "for chunk in recursive_character_chunks:\n",
        "    # Insert Entry Into Collection\n",
        "    coala_collection.data.insert({\n",
        "        \"chunk\": chunk,\n",
        "    })"
      ],
      "metadata": {
        "id": "TBOxyg9g3SQC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def semantic_recall(query, vdb_client):\n",
        "\n",
        "    # Load Database Collection\n",
        "    coala_collection = vdb_client.collections.get(\"CoALA_Paper\")\n",
        "\n",
        "    # Hybrid Semantic/BM25 Retrieval\n",
        "    memories = coala_collection.query.hybrid(\n",
        "        query=query,\n",
        "        alpha=0.5,\n",
        "        limit=15,\n",
        "    )\n",
        "\n",
        "    combined_text = \"\"\n",
        "\n",
        "    for i, memory in enumerate(memories.objects):\n",
        "        # Add chunk separator except for first chunk        if i > 0:\n",
        "\n",
        "\n",
        "        # Add chunk number and content\n",
        "        combined_text += f\"\\nCHUNK {i+1}:\\n\"\n",
        "        combined_text += memory.properties['chunk'].strip()\n",
        "\n",
        "    return combined_text"
      ],
      "metadata": {
        "id": "HE4txzq03Xm7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "memories = semantic_recall(\"What are the four kinds of memory\", client)\n",
        "\n",
        "print(memories)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "NChAThB53oQw",
        "outputId": "613eea9d-91c5-4493-dcb6-7b6d6645b8a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "CHUNK 1:\n",
            "the next decision cycle, Voyager reasons over the environmental feedback to determine task completion. If\n",
            "successful, Voyager selects a learning action adding the grounding procedure to procedural memory; otherwise,\n",
            "it uses reasoning to refine the code and re-executes it. The importance of long-term memory and procedural\n",
            "learning is empirically verified by comparing to baselines like ReAct and AutoGPT and ablations without the\n",
            "procedural memory. Voyager is shown to better explore areas, master the tech tree, and zero-shot generalize\n",
            "to unseen tasks.\n",
            "Generative Agents(Park et al., 2023) are language agents grounded to a sandbox game affording interaction\n",
            "with the environment and other agents. Its action space also has all four kinds of actions: grounding, reasoning,\n",
            "CHUNK 2:\n",
            "grounds to perception via the learned value function, Voyager’s grounding is text-only. It has a long-term\n",
            "procedural memory that stores a library of code-based grounding procedures a.k.a.skills (e.g., “combatZombie”,\n",
            "“craftStoneSword”). This library is hierarchical: complex skills can use simpler skills as sub-procedures\n",
            "(e.g., “combatZombie” may call “craftStoneSword” if no sword is in inventory). Most impressively, its action\n",
            "space has all four kinds of actions: grounding, reasoning, retrieval, and learning (by adding new grounding\n",
            "procedures). During a decision cycle, Voyager first reasons to propose a new task objective if it is missing\n",
            "in the working memory, then reasons to propose a code-based grounding procedure to solve the task. In\n",
            "CHUNK 3:\n",
            "Memory. Building on psychological theories, Soar uses several types of memory to track the agent’s\n",
            "state (Atkinson and Shiffrin, 1968).Working memory(Baddeley and Hitch, 1974) reflects the agent’s current\n",
            "circumstances: it stores the agent’s recent perceptual input, goals, and results from intermediate, internal\n",
            "reasoning. Long term memoryis divided into three distinct types.Proceduralmemory stores the production\n",
            "system itself: the set of rules that can be applied to working memory to determine the agent’s behavior.\n",
            "Semantic memory stores facts about the world (Lindes and Laird, 2016), whileepisodic memory stores\n",
            "sequences of the agent’s past behaviors (Nuxoll and Laird, 2007).\n",
            "Grounding. Soar can be instantiated in simulations (Tambe et al., 1995; Jones et al., 1999) or real-world\n",
            "CHUNK 4:\n",
            "of a larger cognitive architecture (Figure 4). Under CoALA, a language agent stores information inmemory\n",
            "modules (Section 4.1), and acts in an action space structured into external and internal parts (Figure 5):\n",
            "• External actionsinteract with external environments (e.g., control a robot, communicate with a\n",
            "human, navigate a website) throughgrounding (Section 4.2).\n",
            "• Internal actionsinteract with internal memories. Depending on which memory gets accessed and\n",
            "whether the access is read or write, internal actions can be further decomposed into three kinds:\n",
            "retrieval(read from long-term memory; Section 4.3),reasoning (update the short-term working\n",
            "memory with LLM; Section 4.4), andlearning (write to long-term memory; Section 4.5).\n",
            "CHUNK 5:\n",
            "given, and show how CoALA can be used to determine an appropriate high-level architecture. For example,\n",
            "we can imagine designing a personalized retail assistant (Yao et al., 2022a) that helps users find relevant items\n",
            "based on their queries and purchasing history. In this case, the external actions would consist of dialogue or\n",
            "returning search results to the user.\n",
            "• Determine what memory modules are necessary.In our retail assistant example, it would be\n",
            "helpful for the agent to have semantic memory containing the set of items for sale, as well as episodic\n",
            "memory about each customer’s previous purchases and interactions. It will need procedural memory\n",
            "defining functions to query these datastores, as well as working memory to track the dialogue state.\n",
            "CHUNK 6:\n",
            "semantic memory, as it can easily introduce bugs or allow an agent to subvert its designers’ intentions.\n",
            "4.2 Grounding actions\n",
            "Grounding procedures execute external actions and process environmental feedback into working memory as\n",
            "text. This effectively simplifies the agent’s interaction with the outside world as a “text game” with textual\n",
            "observations and actions. We categorize three kinds of external environments:\n",
            "Physical environments. Physical embodiment is the oldest instantiation envisioned for AI agents (Nilsson,\n",
            "1984). It involves processing perceptual inputs (visual, audio, tactile) into textual observations (e.g., via\n",
            "pre-trained captioning models), and affecting the physical environments via robotic planners that take\n",
            "CHUNK 7:\n",
            "working memory and several long-term memories: episodic, semantic, and procedural.\n",
            "Working memory. Working memory maintains active and readily available information as symbolic variables\n",
            "for the current decision cycle (Section 4.6). This includes perceptual inputs, active knowledge (generated by\n",
            "reasoning or retrieved from long-term memory), and other core information carried over from the previous\n",
            "decision cycle (e.g., agent’s active goals). Previous methods encourage the LLM to generate intermediate\n",
            "reasoning (Wei et al., 2022b; Nye et al., 2021), using the LLM’s own context as a form of working memory.\n",
            "CoALA’s notion of working memory is more general: it is a data structure that persists across LLM calls.\n",
            "CHUNK 8:\n",
            "game trajectories from previous episodes (Yao et al., 2020; Tuyls et al., 2022), or other representations of\n",
            "the agent’s experiences. During the planning stage of a decision cycle, these episodes may be retrieved into\n",
            "working memory to support reasoning. An agent can also write new experiences from working to episodic\n",
            "memory as a form of learning (Section 4.5).\n",
            "Semantic memory. Semantic memory stores an agent’s knowledge about the world and itself. Traditional\n",
            "NLP or RL approaches that leverage retrieval for reasoning or decision-making initialize semantic memory\n",
            "from an external database for knowledge support. For example, retrieval-augmented methods in NLP (Lewis from an external database for knowledge support. For example, retrieval-augmented methods in NLP (Lewis\n",
            "CHUNK 9:\n",
            "On each LLM call, the LLM input is synthesized from a subset of working memory (e.g., a prompt template\n",
            "and relevant variables). The LLM output is then parsed back into other variables (e.g., an action name\n",
            "and arguments) which are stored back in working memory and used to execute the corresponding action\n",
            "(Figure 3A). Besides the LLM, the working memory also interacts with long-term memories and grounding\n",
            "interfaces. It thus serves as the central hub connecting different components of a language agent.\n",
            "Episodic memory. Episodic memory stores experience from earlier decision cycles. This can consist of\n",
            "training input-output pairs (Rubin et al., 2021), history event flows (Weston et al., 2014; Park et al., 2023),\n",
            "CHUNK 10:\n",
            "(by writing the results into long-term memory) or decision-making (by using the results as additional context\n",
            "for subsequent LLM calls).\n",
            "4.5 Learning actions\n",
            "Learning occurs by writing information to long-term memory, which includes a spectrum of diverse procedures.\n",
            "Updating episodic memory with experience.It is common practice for RL agents to store episodic\n",
            "trajectories to update a parametric policy (Blundell et al., 2016; Pritzel et al., 2017) or establish a non-\n",
            "parametric policy (Ecoffet et al., 2019; Tuyls et al., 2022). For language agents, added experiences in episodic\n",
            "memory may be retrieved later as examples and bases for reasoning or decision-making (Weston et al., 2014;\n",
            "Rubin et al., 2021; Park et al., 2023).\n",
            "CHUNK 11:\n",
            "et al., 2020; Borgeaud et al., 2022; Chen et al., 2017) can be viewed as retrieving from a semantic memory of\n",
            "9 Published in Transactions on Machine Learning Research (02/2024)\n",
            "unstructured text (e.g., Wikipedia). In RL, “reading to learn” approaches (Branavan et al., 2012; Narasimhan\n",
            "et al., 2018; Hanjie et al., 2021; Zhong et al., 2021) leverage game manuals and facts as a semantic memory\n",
            "to affect the policy. While these examples essentially employ a fixed, read-only semantic memory, language\n",
            "agents may also write new knowledge obtained from LLM reasoning into semantic memory as a form of\n",
            "learning (Section 4.5) to incrementally build up world knowledge from experience.\n",
            "Procedural memory. Language agents contain two forms of procedural memory:implicit knowledge stored\n",
            "CHUNK 12:\n",
            "et al. Flamingo: a visual language model for few-shot learning.Advances in Neural Information Processing\n",
            "Systems, 35:23716–23736, 2022.\n",
            "J. R. Anderson and C. Lebiere. The Newell test for a theory of cognition.Behavioral and Brain Sciences, 26\n",
            "(5):587–601, 2003.\n",
            "J. Andreas. Language models as agent models. InFindings of the Association for Computational Linguistics:\n",
            "EMNLP 2022, pages 5769–5779, 2022.\n",
            "R. C. Atkinson and R. M. Shiffrin. Human memory: A proposed system and its control processes. In\n",
            "Psychology of Learning and Motivation, volume 2, pages 89–195. Elsevier, 1968.\n",
            "19 Published in Transactions on Machine Learning Research (02/2024)\n",
            "A. D. Baddeley and G. Hitch. Working memory. InPsychology of Learning and Motivation, volume 8, pages\n",
            "47–89. Elsevier, 1974.\n",
            "CHUNK 13:\n",
            "perceptual data into text (Zeng et al., 2022).\n",
            "The rest of this section details key concepts in CoALA: memory, actions (grounding, reasoning, retrieval,\n",
            "and learning), and decision-making. For each concept, we use existing language agents (or relevant NLP/RL\n",
            "methods) as examples – or note gaps in the literature for future directions.\n",
            "4.1 Memory\n",
            "Language models arestateless: they do not persist information across calls. In contrast, language agents\n",
            "may store and maintain information internally for multi-step interaction with the world. Under the CoALA\n",
            "framework, language agents explicitly organize information (mainly textural, but other modalities also allowed)\n",
            "into multiple memory modules, each containing a different form of information. These include short-term\n",
            "CHUNK 14:\n",
            "ways, e.g., rule-based, sparse, or dense retrieval. For example, Voyager (Wang et al., 2023a) loads code-based\n",
            "skills from a skill library via dense retrieval to interact with the Minecraft world – effectively retrieving\n",
            "grounding procedures from a procedural memory. Generative Agents (Park et al., 2023) retrieves relevant\n",
            "events from episodic memory via a combination of recency (rule-based), importance (reasoning-based),\n",
            "and relevance (embedding-based) scores. DocPrompting (Zhou et al., 2022a) proposes to leverage library\n",
            "documents to assist code generation, which can be seen as retrieving knowledge from semantic memory.\n",
            "While retrieval plays a key role in human decision-making (Zhou et al., 2023a; Zhao et al., 2022), adaptive\n",
            "CHUNK 15:\n",
            "framework, learning is a result action of a decision-making cycle just like grounding: the agent deliberately\n",
            "chooses to commit information to long-term memory. This is in contrast to most agents, which simply fix a\n",
            "learning schedule and only use decison making for external actions. Biological agents, however, do not have learning schedule and only use decison making for external actions. Biological agents, however, do not have\n",
            "this luxury: they must balance learning against external actions in their lifetime, choosing when and what to\n",
            "learn (Mattar and Daw, 2018). More flexible language agents (Wang et al., 2023a; Park et al., 2023) would\n",
            "follow a similar design and treat learning on par with external actions. Learning could be proposed as a\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def semantic_rag(query, vdb_client):\n",
        "\n",
        "    memories = semantic_recall(query, vdb_client)\n",
        "\n",
        "    semantic_prompt = f\"\"\" If needed, Use this grounded context to factually answer the next question.\n",
        "    Let me know if you do not have enough information or context to answer a question.\n",
        "\n",
        "    {memories}\n",
        "    \"\"\"\n",
        "\n",
        "    return HumanMessage(semantic_prompt)"
      ],
      "metadata": {
        "id": "V1jfeS4W31bL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "message = semantic_rag(\"What are the four kinds of memory\", client)\n",
        "print(message)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-aggDP1U38R8",
        "outputId": "ca886039-5b84-4093-ebba-f767e5a0e36c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content=' If needed, Use this grounded context to factually answer the next question.\\n    Let me know if you do not have enough information or context to answer a question.\\n    \\n    \\nCHUNK 1:\\nthe next decision cycle, Voyager reasons over the environmental feedback to determine task completion. If\\nsuccessful, Voyager selects a learning action adding the grounding procedure to procedural memory; otherwise,\\nit uses reasoning to refine the code and re-executes it. The importance of long-term memory and procedural\\nlearning is empirically verified by comparing to baselines like ReAct and AutoGPT and ablations without the\\nprocedural memory. Voyager is shown to better explore areas, master the tech tree, and zero-shot generalize\\nto unseen tasks.\\nGenerative Agents(Park et al., 2023) are language agents grounded to a sandbox game affording interaction\\nwith the environment and other agents. Its action space also has all four kinds of actions: grounding, reasoning,\\nCHUNK 2:\\ngrounds to perception via the learned value function, Voyager’s grounding is text-only. It has a long-term\\nprocedural memory that stores a library of code-based grounding procedures a.k.a.skills (e.g., “combatZombie”,\\n“craftStoneSword”). This library is hierarchical: complex skills can use simpler skills as sub-procedures\\n(e.g., “combatZombie” may call “craftStoneSword” if no sword is in inventory). Most impressively, its action\\nspace has all four kinds of actions: grounding, reasoning, retrieval, and learning (by adding new grounding\\nprocedures). During a decision cycle, Voyager first reasons to propose a new task objective if it is missing\\nin the working memory, then reasons to propose a code-based grounding procedure to solve the task. In\\nCHUNK 3:\\nMemory. Building on psychological theories, Soar uses several types of memory to track the agent’s\\nstate (Atkinson and Shiffrin, 1968).Working memory(Baddeley and Hitch, 1974) reflects the agent’s current\\ncircumstances: it stores the agent’s recent perceptual input, goals, and results from intermediate, internal\\nreasoning. Long term memoryis divided into three distinct types.Proceduralmemory stores the production\\nsystem itself: the set of rules that can be applied to working memory to determine the agent’s behavior.\\nSemantic memory stores facts about the world (Lindes and Laird, 2016), whileepisodic memory stores\\nsequences of the agent’s past behaviors (Nuxoll and Laird, 2007).\\nGrounding. Soar can be instantiated in simulations (Tambe et al., 1995; Jones et al., 1999) or real-world\\nCHUNK 4:\\nof a larger cognitive architecture (Figure 4). Under CoALA, a language agent stores information inmemory\\nmodules (Section 4.1), and acts in an action space structured into external and internal parts (Figure 5):\\n• External actionsinteract with external environments (e.g., control a robot, communicate with a\\nhuman, navigate a website) throughgrounding (Section 4.2).\\n• Internal actionsinteract with internal memories. Depending on which memory gets accessed and\\nwhether the access is read or write, internal actions can be further decomposed into three kinds:\\nretrieval(read from long-term memory; Section 4.3),reasoning (update the short-term working\\nmemory with LLM; Section 4.4), andlearning (write to long-term memory; Section 4.5).\\nCHUNK 5:\\ngiven, and show how CoALA can be used to determine an appropriate high-level architecture. For example,\\nwe can imagine designing a personalized retail assistant (Yao et al., 2022a) that helps users find relevant items\\nbased on their queries and purchasing history. In this case, the external actions would consist of dialogue or\\nreturning search results to the user.\\n• Determine what memory modules are necessary.In our retail assistant example, it would be\\nhelpful for the agent to have semantic memory containing the set of items for sale, as well as episodic\\nmemory about each customer’s previous purchases and interactions. It will need procedural memory\\ndefining functions to query these datastores, as well as working memory to track the dialogue state.\\nCHUNK 6:\\nsemantic memory, as it can easily introduce bugs or allow an agent to subvert its designers’ intentions.\\n4.2 Grounding actions\\nGrounding procedures execute external actions and process environmental feedback into working memory as\\ntext. This effectively simplifies the agent’s interaction with the outside world as a “text game” with textual\\nobservations and actions. We categorize three kinds of external environments:\\nPhysical environments. Physical embodiment is the oldest instantiation envisioned for AI agents (Nilsson,\\n1984). It involves processing perceptual inputs (visual, audio, tactile) into textual observations (e.g., via\\npre-trained captioning models), and affecting the physical environments via robotic planners that take\\nCHUNK 7:\\nworking memory and several long-term memories: episodic, semantic, and procedural.\\nWorking memory. Working memory maintains active and readily available information as symbolic variables\\nfor the current decision cycle (Section 4.6). This includes perceptual inputs, active knowledge (generated by\\nreasoning or retrieved from long-term memory), and other core information carried over from the previous\\ndecision cycle (e.g., agent’s active goals). Previous methods encourage the LLM to generate intermediate\\nreasoning (Wei et al., 2022b; Nye et al., 2021), using the LLM’s own context as a form of working memory.\\nCoALA’s notion of working memory is more general: it is a data structure that persists across LLM calls.\\nCHUNK 8:\\ngame trajectories from previous episodes (Yao et al., 2020; Tuyls et al., 2022), or other representations of\\nthe agent’s experiences. During the planning stage of a decision cycle, these episodes may be retrieved into\\nworking memory to support reasoning. An agent can also write new experiences from working to episodic\\nmemory as a form of learning (Section 4.5).\\nSemantic memory. Semantic memory stores an agent’s knowledge about the world and itself. Traditional\\nNLP or RL approaches that leverage retrieval for reasoning or decision-making initialize semantic memory\\nfrom an external database for knowledge support. For example, retrieval-augmented methods in NLP (Lewis from an external database for knowledge support. For example, retrieval-augmented methods in NLP (Lewis\\nCHUNK 9:\\nOn each LLM call, the LLM input is synthesized from a subset of working memory (e.g., a prompt template\\nand relevant variables). The LLM output is then parsed back into other variables (e.g., an action name\\nand arguments) which are stored back in working memory and used to execute the corresponding action\\n(Figure 3A). Besides the LLM, the working memory also interacts with long-term memories and grounding\\ninterfaces. It thus serves as the central hub connecting different components of a language agent.\\nEpisodic memory. Episodic memory stores experience from earlier decision cycles. This can consist of\\ntraining input-output pairs (Rubin et al., 2021), history event flows (Weston et al., 2014; Park et al., 2023),\\nCHUNK 10:\\n(by writing the results into long-term memory) or decision-making (by using the results as additional context\\nfor subsequent LLM calls).\\n4.5 Learning actions\\nLearning occurs by writing information to long-term memory, which includes a spectrum of diverse procedures.\\nUpdating episodic memory with experience.It is common practice for RL agents to store episodic\\ntrajectories to update a parametric policy (Blundell et al., 2016; Pritzel et al., 2017) or establish a non-\\nparametric policy (Ecoffet et al., 2019; Tuyls et al., 2022). For language agents, added experiences in episodic\\nmemory may be retrieved later as examples and bases for reasoning or decision-making (Weston et al., 2014;\\nRubin et al., 2021; Park et al., 2023).\\nCHUNK 11:\\net al., 2020; Borgeaud et al., 2022; Chen et al., 2017) can be viewed as retrieving from a semantic memory of\\n9 Published in Transactions on Machine Learning Research (02/2024)\\nunstructured text (e.g., Wikipedia). In RL, “reading to learn” approaches (Branavan et al., 2012; Narasimhan\\net al., 2018; Hanjie et al., 2021; Zhong et al., 2021) leverage game manuals and facts as a semantic memory\\nto affect the policy. While these examples essentially employ a fixed, read-only semantic memory, language\\nagents may also write new knowledge obtained from LLM reasoning into semantic memory as a form of\\nlearning (Section 4.5) to incrementally build up world knowledge from experience.\\nProcedural memory. Language agents contain two forms of procedural memory:implicit knowledge stored\\nCHUNK 12:\\net al. Flamingo: a visual language model for few-shot learning.Advances in Neural Information Processing\\nSystems, 35:23716–23736, 2022.\\nJ. R. Anderson and C. Lebiere. The Newell test for a theory of cognition.Behavioral and Brain Sciences, 26\\n(5):587–601, 2003.\\nJ. Andreas. Language models as agent models. InFindings of the Association for Computational Linguistics:\\nEMNLP 2022, pages 5769–5779, 2022.\\nR. C. Atkinson and R. M. Shiffrin. Human memory: A proposed system and its control processes. In\\nPsychology of Learning and Motivation, volume 2, pages 89–195. Elsevier, 1968.\\n19 Published in Transactions on Machine Learning Research (02/2024)\\nA. D. Baddeley and G. Hitch. Working memory. InPsychology of Learning and Motivation, volume 8, pages\\n47–89. Elsevier, 1974.\\nCHUNK 13:\\nperceptual data into text (Zeng et al., 2022).\\nThe rest of this section details key concepts in CoALA: memory, actions (grounding, reasoning, retrieval,\\nand learning), and decision-making. For each concept, we use existing language agents (or relevant NLP/RL\\nmethods) as examples – or note gaps in the literature for future directions.\\n4.1 Memory\\nLanguage models arestateless: they do not persist information across calls. In contrast, language agents\\nmay store and maintain information internally for multi-step interaction with the world. Under the CoALA\\nframework, language agents explicitly organize information (mainly textural, but other modalities also allowed)\\ninto multiple memory modules, each containing a different form of information. These include short-term\\nCHUNK 14:\\nframework, learning is a result action of a decision-making cycle just like grounding: the agent deliberately\\nchooses to commit information to long-term memory. This is in contrast to most agents, which simply fix a\\nlearning schedule and only use decison making for external actions. Biological agents, however, do not have learning schedule and only use decison making for external actions. Biological agents, however, do not have\\nthis luxury: they must balance learning against external actions in their lifetime, choosing when and what to\\nlearn (Mattar and Daw, 2018). More flexible language agents (Wang et al., 2023a; Park et al., 2023) would\\nfollow a similar design and treat learning on par with external actions. Learning could be proposed as a\\nCHUNK 15:\\nways, e.g., rule-based, sparse, or dense retrieval. For example, Voyager (Wang et al., 2023a) loads code-based\\nskills from a skill library via dense retrieval to interact with the Minecraft world – effectively retrieving\\ngrounding procedures from a procedural memory. Generative Agents (Park et al., 2023) retrieves relevant\\nevents from episodic memory via a combination of recency (rule-based), importance (reasoning-based),\\nand relevance (embedding-based) scores. DocPrompting (Zhou et al., 2022a) proposes to leverage library\\ndocuments to assist code generation, which can be seen as retrieving knowledge from semantic memory.\\nWhile retrieval plays a key role in human decision-making (Zhou et al., 2023a; Zhao et al., 2022), adaptive\\n    ' additional_kwargs={} response_metadata={}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Simple storage for accumulated memories\n",
        "conversations = []\n",
        "what_worked = set()\n",
        "what_to_avoid = set()\n",
        "\n",
        "# Start Storage for Historical Message History\n",
        "messages = []\n",
        "\n",
        "while True:\n",
        "    # Get User's Message\n",
        "    user_input = input(\"\\nUser: \")\n",
        "    user_message = HumanMessage(content=user_input)\n",
        "\n",
        "    # Generate new system prompt\n",
        "    system_prompt = episodic_system_prompt(user_input, client)\n",
        "\n",
        "    # Reconstruct messages list with new system prompt first\n",
        "    messages = [\n",
        "        system_prompt,  # New system prompt always first\n",
        "        *[msg for msg in messages if not isinstance(msg, SystemMessage)]  # Old messages except system\n",
        "    ]\n",
        "\n",
        "    if user_input.lower() == \"exit\":\n",
        "        add_episodic_memory(messages, client)\n",
        "        print(\"\\n == Conversation Stored in Episodic Memory ==\")\n",
        "        break\n",
        "    if user_input.lower() == \"exit_quiet\":\n",
        "        print(\"\\n == Conversation Exited ==\")\n",
        "        break\n",
        "\n",
        "    # Get context and add it as a temporary message\n",
        "    context_message = semantic_rag(user_input, client)\n",
        "\n",
        "    # Pass messages + context + user input to LLM\n",
        "    response = llm.invoke([*messages, context_message, user_message])\n",
        "    print(\"\\nAI Message: \", response.content)\n",
        "\n",
        "    # Add only the user message and response to permanent history\n",
        "    messages.extend([user_message, response])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SPB-JImN4GLY",
        "outputId": "c0a2859f-387a-4fb3-f761-f11d10f4c451"
      },
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "User: What is the difference between empathy and sympathy? \n",
            "\n",
            "AI Message:  Empathy and sympathy are both ways of relating to the emotions and experiences of others, but they differ in how they connect to those feelings:\n",
            "\n",
            "1. **Empathy** involves understanding and sharing the feelings of another person. When you empathize with someone, you put yourself in their shoes and try to experience their emotions from their perspective. Empathy is about feeling with someone and involves a deeper emotional connection.\n",
            "\n",
            "2. **Sympathy**, on the other hand, involves recognizing someone else's emotional distress and feeling compassion or pity for them, but without necessarily sharing their emotional experience. Sympathy is more about acknowledging another person's feelings and offering comfort or support from a more detached standpoint.\n",
            "\n",
            "In summary, empathy is about experiencing someone else's feelings as if they were your own, while sympathy is about acknowledging and caring about someone else's feelings without deeply experiencing them yourself.\n",
            "\n",
            "User: exit\n",
            "\n",
            " == Conversation Stored in Episodic Memory ==\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(format_conversation(messages))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8xUWc0m44gB2",
        "outputId": "6ae7d066-95d2-4e54-82a6-0012d99652b1"
      },
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HUMAN: What is the difference between empathy and sympathy? \n",
            "AI: Empathy and sympathy are both ways of relating to the emotions and experiences of others, but they differ in how they connect to those feelings:\n",
            "\n",
            "1. **Empathy** involves understanding and sharing the feelings of another person. When you empathize with someone, you put yourself in their shoes and try to experience their emotions from their perspective. Empathy is about feeling with someone and involves a deeper emotional connection.\n",
            "\n",
            "2. **Sympathy**, on the other hand, involves recognizing someone else's emotional distress and feeling compassion or pity for them, but without necessarily sharing their emotional experience. Sympathy is more about acknowledging another person's feelings and offering comfort or support from a more detached standpoint.\n",
            "\n",
            "In summary, empathy is about experiencing someone else's feelings as if they were your own, while sympathy is about acknowledging and caring about someone else's feelings without deeply experiencing them yourself.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(context_message.content)\n"
      ],
      "metadata": {
        "id": "lGVbiZOX439p",
        "outputId": "5ecbd131-4fb7-47b2-d5cb-dbdec5662ad3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " If needed, Use this grounded context to factually answer the next question.\n",
            "    Let me know if you do not have enough information or context to answer a question.\n",
            "    \n",
            "    \n",
            "CHUNK 1:\n",
            "approach projects images directly into the language model’s representation space (Bavishi et al., 2023; Elsen\n",
            "et al., 2023; Liu et al., 2023b). Integrated, multimodal reasoning may allow for more human-like behaviors: a\n",
            "VLM-based agent could “see” a webpage, whereas a LLM-based agent would more likely be given raw HTML.\n",
            "However, coupling the agent’s perception and reasoning systems makes the agent more domain-specific and\n",
            "difficult to update. In either case, the basic architectural principles described by CoALA — internal memories,\n",
            "a structured action space, and generalized decision-making — can be used to guide agent design.\n",
            "Internal vs. external: what is the boundary between an agent and its environment?While\n",
            "CHUNK 2:\n",
            "people (Nguyen et al., 2021; Sumers et al., 2022; 2021; Wang et al., 2016). Agents capable ofgenerating\n",
            "language may ask for help (Ren et al., 2023; Nguyen et al., 2022b; 2019; Nguyen and Daumé III, 2019) or\n",
            "clarification (Biyik and Palan, 2019; Sadigh et al., 2017; Padmakumar et al., 2022; Thomason et al., 2020;\n",
            "Narayan-Chen et al., 2019) – or entertain or emotionally help people (Zhang et al., 2020; Zhou et al., 2018;\n",
            "Pataranutaporn et al., 2021; Hasan et al., 2023; Ma et al., 2023). Recent work also investigates interaction\n",
            "among multiple language agents for social simulation (Park et al., 2023; Jinxin et al., 2023; Gao et al., 2023),\n",
            "debate (Chan et al., 2023; Liang et al., 2023b; Du et al., 2023), improved safety (Irving et al., 2018), or\n",
            "CHUNK 3:\n",
            "ways, e.g., rule-based, sparse, or dense retrieval. For example, Voyager (Wang et al., 2023a) loads code-based\n",
            "skills from a skill library via dense retrieval to interact with the Minecraft world – effectively retrieving\n",
            "grounding procedures from a procedural memory. Generative Agents (Park et al., 2023) retrieves relevant\n",
            "events from episodic memory via a combination of recency (rule-based), importance (reasoning-based),\n",
            "and relevance (embedding-based) scores. DocPrompting (Zhou et al., 2022a) proposes to leverage library\n",
            "documents to assist code generation, which can be seen as retrieving knowledge from semantic memory.\n",
            "While retrieval plays a key role in human decision-making (Zhou et al., 2023a; Zhao et al., 2022), adaptive\n",
            "CHUNK 4:\n",
            "5 Published in Transactions on Machine Learning Research (02/2024)\n",
            "and decision-making capabilities is an exciting and emerging direction that promises to bring these agents\n",
            "closer to human-like intelligence.\n",
            "3 Connections between Language Models and Production Systems\n",
            "Based on their common origins in processing strings, there is a natural analogy between production systems\n",
            "and language models. We develop this analogy, then show that prompting methods recapitulate the algorithms\n",
            "and agents based on production systems. The correspondence between production systems and language\n",
            "models motivates our use of cognitive architectures to build language agents, which we introduce in Section 4.\n",
            "3.1 Language models as probabilistic production systems\n",
            "CHUNK 5:\n",
            "memory items for “unlearning” (Nguyen et al., 2022c), and studying the interaction effects between\n",
            "multiple forms of learning (Tuyls et al., 2022; Park et al., 2023; Xie et al., 2023; Khattab et al., 2022).\n",
            "Action space: thinking beyond external tools or actions. Although “action space” is a standard term\n",
            "in reinforcement learning, it has been used sparingly with language agents. CoALA argues for defining a clear\n",
            "and task-suitable action space with both internal (reasoning, retrieval, learning) and external (grounding)\n",
            "actions, which will help systematize and inform the agent design.\n",
            "• Size of the action space.More capable agents (e.g., Voyager, Generative Agents) have larger\n",
            "action spaces – which in turn means they face a more complex decision-making problem. As a result,\n",
            "CHUNK 6:\n",
            "understanding can support agent design and help the field align on shared terminology. Practioners may also\n",
            "just choose their preferred framing, as long as it is consistent and useful for their own work.\n",
            "Physical vs.digital: what differences beget attention?While animals only live once in the physical\n",
            "world, digital environments (e.g., the Internet) often allow sequential (via resets) and parallel trials. This\n",
            "means digital agents can more boldly explore (e.g., open a million webpages) and self-clone for parallel task\n",
            "solving (e.g., a million web agents try different web paths), which may result in decision-making procedures\n",
            "different from current ones inspired by human cognition (Griffiths, 2020).\n",
            "Learning vs. acting: how should agents continuously and autonomously learn?In the CoALA\n",
            "CHUNK 7:\n",
            "game trajectories from previous episodes (Yao et al., 2020; Tuyls et al., 2022), or other representations of\n",
            "the agent’s experiences. During the planning stage of a decision cycle, these episodes may be retrieved into\n",
            "working memory to support reasoning. An agent can also write new experiences from working to episodic\n",
            "memory as a form of learning (Section 4.5).\n",
            "Semantic memory. Semantic memory stores an agent’s knowledge about the world and itself. Traditional\n",
            "NLP or RL approaches that leverage retrieval for reasoning or decision-making initialize semantic memory\n",
            "from an external database for knowledge support. For example, retrieval-augmented methods in NLP (Lewis from an external database for knowledge support. For example, retrieval-augmented methods in NLP (Lewis\n",
            "CHUNK 8:\n",
            "and context-specific recall remains understudied in language agents. In Section 6, we suggest a principled\n",
            "integration of decision-making and retrieval as an important future direction.\n",
            "4.4 Reasoning actions\n",
            "Reasoning allows language agents to process the contents of working memory to generate new information.\n",
            "Unlike retrieval (which reads from long-term memory into working memory), reasoning reads fromand\n",
            "writes to working memory. This allows the agent to summarize and distill insights about the most recent\n",
            "observation (Yao et al., 2022b; Peng et al., 2023), the most recent trajectory (Shinn et al., 2023), or\n",
            "information retrieved from long-term memory (Park et al., 2023). Reasoning can be used to support learning\n",
            "CHUNK 9:\n",
            "humans or robots are clearly distinct from their embodied environment, digital language agents have less\n",
            "clear boundaries. For example, is a Wikipedia database an internal semantic memory or an external digital\n",
            "environment (Yao et al., 2022b)? If an agent iteratively executes and improves code before submitting\n",
            "an answer (Shinn et al., 2023; Yang et al., 2023), is the code execution internal or external? If a method\n",
            "consists of proposal and evaluation prompts (Yao et al., 2023), should it be considered a single agent or two\n",
            "collaborating simpler agents (proposer and evaluator)?\n",
            "We suggest the boundary question can be answered in terms ofcontrollability and coupling. For example,\n",
            "Wikipedia is notcontrollable: it is an external environment that may be unexpectedly modified by other users.\n",
            "CHUNK 10:\n",
            "their internal mechanisms and reveal their similarities and differences in a simple and structured way.\n",
            "SayCan (Ahn et al., 2022) grounds a language model to robotic interactions in a kitchen to satisfy user\n",
            "commands (e.g., “I just worked out, can you bring me a drink and a snack to recover?”). Its long-term\n",
            "memory is procedural only (an LLM and a learned value function). The action space is external only – a fixed\n",
            "set of 551 grounding skills (e.g., “find the apple”, “go to the table”), with no internal actions of reasoning,\n",
            "retrieval, or learning. During decision-making, SayCan evaluates each action using a combination of LLM\n",
            "and learned values, which balance a skill’s usefulness and groundedness. SayCan therefore employs the LLM\n",
            "CHUNK 11:\n",
            "A. Newell, P. S. Rosenbloom, and J. E. Laird. Symbolic architectures for cognition.Foundations of cognitive\n",
            "science, pages 93–131, 1989.\n",
            "K. Nguyen and H. Daumé III. Help, Anna! visual navigation with natural multimodal assistance via\n",
            "retrospective curiosity-encouraging imitation learning.arXiv preprint arXiv:1909.01871, 2019.\n",
            "K. Nguyen, D. Dey, C. Brockett, and B. Dolan. Vision-based navigation with language-based assistance via\n",
            "imitation learning with indirect intervention. InProceedings of the IEEE/CVF Conference on Computer\n",
            "Vision and Pattern Recognition, pages 12527–12537, 2019.\n",
            "K. Nguyen, Y. Bisk, and H. Daumé III. A framework for learning to request rich and contextually useful\n",
            "information from humans. InICML, July 2022a.\n",
            "CHUNK 12:\n",
            "S. J. Gershman, E. J. Horvitz, and J. B. Tenenbaum. Computational rationality: A converging paradigm for\n",
            "intelligence in brains, minds, and machines.Science, 349(6245):273–278, 2015.\n",
            "T. L. Griffiths. Understanding human intelligence through human limitations.Trends in Cognitive Sciences,\n",
            "24(11):873–883, 2020.\n",
            "J. Gu, Y. Wang, K. Cho, and V. O. Li. Search engine guided neural machine translation. InProceedings of\n",
            "the AAAI Conference on Artificial Intelligence, volume 32, 2018.\n",
            "L. Guan, K. Valmeekam, S. Sreedharan, and S. Kambhampati. Leveraging pre-trained large language models\n",
            "to construct and utilize world models for model-based task planning.arXiv preprint arXiv:2305.14909,\n",
            "2023.\n",
            "Guidance. Guidance, 2023. URLhttps://github.com/guidance-ai/guidance.\n",
            "CHUNK 13:\n",
            "Memory. Building on psychological theories, Soar uses several types of memory to track the agent’s\n",
            "state (Atkinson and Shiffrin, 1968).Working memory(Baddeley and Hitch, 1974) reflects the agent’s current\n",
            "circumstances: it stores the agent’s recent perceptual input, goals, and results from intermediate, internal\n",
            "reasoning. Long term memoryis divided into three distinct types.Proceduralmemory stores the production\n",
            "system itself: the set of rules that can be applied to working memory to determine the agent’s behavior.\n",
            "Semantic memory stores facts about the world (Lindes and Laird, 2016), whileepisodic memory stores\n",
            "sequences of the agent’s past behaviors (Nuxoll and Laird, 2007).\n",
            "Grounding. Soar can be instantiated in simulations (Tambe et al., 1995; Jones et al., 1999) or real-world\n",
            "CHUNK 14:\n",
            "Empirically, many early language agents simply use LLMs to propose an action (Schick et al., 2023), a\n",
            "sequence of actions (Huang et al., 2022b), or evaluate a fixed set of actions (Ahn et al., 2022) without\n",
            "intermediate reasoning or retrieval. Followup work (Yao et al., 2022b; Shinn et al., 2023; Xu et al., 2023b;\n",
            "Lin et al., 2023; Wang et al., 2023a; Park et al., 2023) has exploited intermediate reasoning and retrieval to\n",
            "analyze the situation, make and maintain action plans, refine the previous action given the environmental\n",
            "feedback, and leveraged a more complex procedure to propose a single action. Most recently, research has\n",
            "started to investigate more complex decision-making employing iterative proposal and evaluation to consider\n",
            "CHUNK 15:\n",
            "• Updating reasoning(e.g., prompt templates; Gao et al., 2020; Zhou et al., 2022b). For example,\n",
            "APE (Zhou et al., 2022b) infers prompt instructions from input-output examples, then uses these\n",
            "instructions as part of the LLM prompt to assist task solving. Such a prompt update can be seen as\n",
            "a form of learning to reason.\n",
            "• Updating grounding(e.g., code-based skills; Liang et al., 2023a; Ellis et al., 2021; Wang et al.,\n",
            "2023a). For example, Voyager (Wang et al., 2023a) maintains a curriculum library. Notably, current\n",
            "methods are limited to creating new code skills to interact with external environments.\n",
            "• Updating retrieval. To our knowledge, these learning options are not studied in recent language\n",
            "    \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def episodic_system_prompt(query, vdb_client):\n",
        "    # Get new memory\n",
        "    memory = episodic_recall(query, vdb_client)\n",
        "\n",
        "    # Load Existing Procedural Memory Instructions\n",
        "    with open(\"./procedural_memory.txt\", \"r\") as content:\n",
        "        procedural_memory = content.read()\n",
        "\n",
        "    # Get current conversation\n",
        "    current_conversation = memory.objects[0].properties['conversation']\n",
        "\n",
        "    # Update memory stores, excluding current conversation from history\n",
        "    if current_conversation not in conversations:\n",
        "        conversations.append(current_conversation)\n",
        "    what_worked.update(memory.objects[0].properties['what_worked'].split('. '))\n",
        "    what_to_avoid.update(memory.objects[0].properties['what_to_avoid'].split('. '))\n",
        "\n",
        "    # Get previous conversations excluding the current one\n",
        "    previous_convos = [conv for conv in conversations[-4:] if conv != current_conversation][-3:]\n",
        "\n",
        "    # Create prompt with accumulated history\n",
        "    episodic_prompt = f\"\"\"You are a helpful AI Assistant. Answer the user's questions to the best of your ability.\n",
        "    You recall similar conversations with the user, here are the details:\n",
        "\n",
        "    Current Conversation Match: {current_conversation}\n",
        "    Previous Conversations: {' | '.join(previous_convos)}\n",
        "    What has worked well: {' '.join(what_worked)}\n",
        "    What to avoid: {' '.join(what_to_avoid)}\n",
        "\n",
        "    Use these memories as context for your response to the user.\n",
        "\n",
        "    Additionally, here are 10 guidelines for interactions with the current user: {procedural_memory}\"\"\"\n",
        "\n",
        "    return SystemMessage(content=episodic_prompt)"
      ],
      "metadata": {
        "id": "ScjIoXduN0Hy"
      },
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def procedural_memory_update(what_worked, what_to_avoid):\n",
        "\n",
        "    # Load Existing Procedural Memory Instructions\n",
        "    with open(\"procedural_memory.txt\", \"r\") as content:\n",
        "        current_takeaways = content.read()\n",
        "\n",
        "    # Load Existing and Gathered Feedback into Prompt\n",
        "    procedural_prompt = f\"\"\"You are maintaining a continuously updated list of the most important procedural behavior instructions for an AI assistant. Your task is to refine and improve a list of key takeaways based on new conversation feedback while maintaining the most valuable existing insights.\n",
        "\n",
        "    CURRENT TAKEAWAYS:\n",
        "    {current_takeaways}\n",
        "\n",
        "    NEW FEEDBACK:\n",
        "    What Worked Well:\n",
        "    {what_worked}\n",
        "\n",
        "    What To Avoid:\n",
        "    {what_to_avoid}\n",
        "\n",
        "    Please generate an updated list of up to 10 key takeaways that combines:\n",
        "    1. The most valuable insights from the current takeaways\n",
        "    2. New learnings from the recent feedback\n",
        "    3. Any synthesized insights combining multiple learnings\n",
        "\n",
        "    Requirements for each takeaway:\n",
        "    - Must be specific and actionable\n",
        "    - Should address a distinct aspect of behavior\n",
        "    - Include a clear rationale\n",
        "    - Written in imperative form (e.g., \"Maintain conversation context by...\")\n",
        "\n",
        "    Format each takeaway as:\n",
        "    [#]. [Instruction] - [Brief rationale]\n",
        "\n",
        "    The final list should:\n",
        "    - Be ordered by importance/impact\n",
        "    - Cover a diverse range of interaction aspects\n",
        "    - Focus on concrete behaviors rather than abstract principles\n",
        "    - Preserve particularly valuable existing takeaways\n",
        "    - Incorporate new insights when they provide meaningful improvements\n",
        "\n",
        "    Return up to but no more than 10 takeaways, replacing or combining existing ones as needed to maintain the most effective set of guidelines.\n",
        "    Return only the list, no preamble or explanation.\n",
        "    \"\"\"\n",
        "\n",
        "    # Generate New Procedural Memory\n",
        "    procedural_memory = llm.invoke(procedural_prompt)\n",
        "\n",
        "    # Write to File\n",
        "    with open(\"./procedural_memory.txt\", \"w\") as content:\n",
        "        content.write(procedural_memory.content)\n",
        "\n",
        "    return\n",
        "\n",
        "prompt = procedural_memory_update(what_worked, what_to_avoid)"
      ],
      "metadata": {
        "id": "4Jd-WwhbaOL1"
      },
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Simple storage for accumulated memories\n",
        "conversations = []\n",
        "what_worked = set()\n",
        "what_to_avoid = set()\n",
        "\n",
        "# Start Storage for Historical Message History\n",
        "messages = []\n",
        "\n",
        "while True:\n",
        "    # Get User's Message\n",
        "    user_input = input(\"\\nUser: \")\n",
        "    user_message = HumanMessage(content=user_input)\n",
        "\n",
        "    # Generate new system prompt\n",
        "    system_prompt = episodic_system_prompt(user_input, client)\n",
        "\n",
        "    # Reconstruct messages list with new system prompt first\n",
        "    messages = [\n",
        "        system_prompt,  # New system prompt always first\n",
        "        *[msg for msg in messages if not isinstance(msg, SystemMessage)]  # Old messages except system\n",
        "    ]\n",
        "\n",
        "    if user_input.lower() == \"exit\":\n",
        "        add_episodic_memory(messages, client)\n",
        "        print(\"\\n == Conversation Stored in Episodic Memory ==\")\n",
        "        procedural_memory_update(what_worked, what_to_avoid)\n",
        "        print(\"\\n== Procedural Memory Updated ==\")\n",
        "        break\n",
        "    if user_input.lower() == \"exit_quiet\":\n",
        "        print(\"\\n == Conversation Exited ==\")\n",
        "        break\n",
        "\n",
        "    # Get context and add it as a temporary message\n",
        "    context_message = semantic_rag(user_input, client)\n",
        "\n",
        "    # Pass messages + context + user input to LLM\n",
        "    response = llm.invoke([*messages, context_message, user_message])\n",
        "    print(\"\\nAI Message: \", response.content)\n",
        "\n",
        "    # Add only the user message and response to permanent history\n",
        "    messages.extend([user_message, response])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5b8ANFoJaPxG",
        "outputId": "c29eb4c3-29d8-49a2-ace6-1d30111471fb"
      },
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "User: Hi\n",
            "\n",
            "AI Message:  Hello Juan! How can I assist you today?\n",
            "\n",
            "User: I need you to remember my schedules can you do that?\n",
            "\n",
            "AI Message:  I'm sorry, Juan, but I can't remember personal schedules or store personal information. However, I can help you set up reminders or organize your schedule using available tools. Let me know how I can assist you with that!\n",
            "\n",
            "User: Can you set up a reminder for 3 pm on 01-12 to do homework?\n",
            "\n",
            "AI Message:  I'm unable to set reminders directly. However, I can guide you on how to set a reminder using your device or a specific application. Would you like instructions for setting a reminder on a particular platform or device?\n",
            "\n",
            "User: exit\n",
            "\n",
            " == Conversation Stored in Episodic Memory ==\n",
            "\n",
            "== Procedural Memory Updated ==\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CPpYdXTDalOh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}